{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Face expression Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import os and get the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\saite\\\\Desktop\\\\face reco'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's assign the dataset path to the basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "base_path = \"C:\\\\Users\\\\saite\\\\Desktop\\\\face reco\\\\images\\\\images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the number of images of each class in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988 angry images\n",
      "436 disgust images\n",
      "4103 fear images\n",
      "7164 happy images\n",
      "4982 neutral images\n",
      "4938 sad images\n",
      "3205 surprise images\n"
     ]
    }
   ],
   "source": [
    "# count number of train images for each expression\n",
    "\n",
    "for expression in os.listdir(base_path + \"\\\\train\"):\n",
    "    print(str(len(os.listdir(base_path + \"\\\\train\\\\\" + expression))) + \" \" + expression + \" images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have good amount of images for each class except for the class disgust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pic_size for the images is (48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_size = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Validation Generator\n",
    "* The color of the images are gray\n",
    "* consider the batch size 128\n",
    "* Shuffle the data for each generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28816 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = ImageDataGenerator()\n",
    "datagen_validation = ImageDataGenerator()\n",
    "\n",
    "train_generator = datagen_train.flow_from_directory(base_path + \"\\\\train\",\n",
    "                                                    target_size=(pic_size,pic_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "validation_generator = datagen_validation.flow_from_directory(base_path + \"\\\\validation\",\n",
    "                                                    target_size=(pic_size,pic_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the basic Library's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "#import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network\n",
    "* I have added 4 convolution layers followed by maxpooling \n",
    "* Increasing the number of channels from 1(input) to 512\n",
    "* input is zero padded with (3,3) \n",
    "* (3,3) filter for each convolution layer\n",
    "* Finally one fully connected layer with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv0/BiasAdd:0\", shape=(None, 52, 52, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (48,48,1)\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Zero-Padding: pads the border of X_input with zeroes\n",
    "X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "# CONV -> BN -> RELU Block applied to X\n",
    "X = Conv2D(16, (3, 3), strides = (1, 1), name = 'conv0')(X)\n",
    "print(X)\n",
    "X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "# MAXPOOL\n",
    "X = MaxPooling2D((2, 2), name='max_pool0')(X)\n",
    "\n",
    "# CONV -> BN -> RELU Block applied to X\n",
    "X = Conv2D(64, (3, 3) , strides = (1, 1), name = 'conv1')(X)\n",
    "X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "# MAXPOOL\n",
    "X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "\n",
    "# CONV -> BN -> RELU Block applied to X\n",
    "X = Conv2D(128, (3, 3), strides = (1, 1), name = 'conv2')(X)\n",
    "X = BatchNormalization(axis = 3, name = 'bn2')(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "# MAXPOOL\n",
    "X = MaxPooling2D((2, 2), name='max_pool2')(X)\n",
    "\n",
    "# CONV -> BN -> RELU Block applied to X\n",
    "X = Conv2D(512, (3 ,3), strides = (1, 1), name = 'conv3')(X)\n",
    "X = BatchNormalization(axis = 3, name = 'bn3')(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "# MAXPOOL\n",
    "X = MaxPooling2D((2, 2), name='max_pool3')(X)\n",
    "\n",
    "\n",
    "\n",
    "# FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "X = Flatten()(X)\n",
    "X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "\n",
    "# Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "model = Model(inputs = X_input, outputs = X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling The Model\n",
    "* Adam Optimizer with 0.01 as learning rate\n",
    "* categorical cross entropy is used as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr = 0.01), loss= tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "225/225 [==============================] - 748s 3s/step - loss: 1.1921e-07 - accuracy: 0.5003 - val_loss: 1.1921e-07 - val_accuracy: 0.4011\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 238s 1s/step - loss: 1.1921e-07 - accuracy: 0.5008 - val_loss: 1.1921e-07 - val_accuracy: 0.4906\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 239s 1s/step - loss: 1.1921e-07 - accuracy: 0.5000 - val_loss: 1.1921e-07 - val_accuracy: 0.4950\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 236s 1s/step - loss: 1.1921e-07 - accuracy: 0.5004 - val_loss: 1.1921e-07 - val_accuracy: 0.4963\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 236s 1s/step - loss: 1.1921e-07 - accuracy: 0.5000 - val_loss: 1.1921e-07 - val_accuracy: 0.4957\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 236s 1s/step - loss: 1.1921e-07 - accuracy: 0.4999 - val_loss: 1.1921e-07 - val_accuracy: 0.4976\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 236s 1s/step - loss: 1.1921e-07 - accuracy: 0.4988 - val_loss: 1.1921e-07 - val_accuracy: 0.4964\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 235s 1s/step - loss: 1.1921e-07 - accuracy: 0.5004 - val_loss: 1.1921e-07 - val_accuracy: 0.4973\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 236s 1s/step - loss: 1.1921e-07 - accuracy: 0.4986 - val_loss: 1.1921e-07 - val_accuracy: 0.4942\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 244s 1s/step - loss: 1.1921e-07 - accuracy: 0.5021 - val_loss: 1.1921e-07 - val_accuracy: 0.4955\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 267s 1s/step - loss: 1.1921e-07 - accuracy: 0.5010 - val_loss: 1.1921e-07 - val_accuracy: 0.4991\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 240s 1s/step - loss: 1.1921e-07 - accuracy: 0.4999 - val_loss: 1.1921e-07 - val_accuracy: 0.4898\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 245s 1s/step - loss: 1.1921e-07 - accuracy: 0.5002 - val_loss: 1.1921e-07 - val_accuracy: 0.4994\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 246s 1s/step - loss: 1.1921e-07 - accuracy: 0.4997 - val_loss: 1.1921e-07 - val_accuracy: 0.4937\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 253s 1s/step - loss: 1.1921e-07 - accuracy: 0.4988 - val_loss: 1.1921e-07 - val_accuracy: 0.5010\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 212s 943ms/step - loss: 1.1921e-07 - accuracy: 0.4995 - val_loss: 1.1921e-07 - val_accuracy: 0.4915\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.5015 - val_loss: 1.1921e-07 - val_accuracy: 0.5041\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.5004 - val_loss: 1.1921e-07 - val_accuracy: 0.4910\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 170s 753ms/step - loss: 1.1921e-07 - accuracy: 0.4998 - val_loss: 1.1921e-07 - val_accuracy: 0.4939\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 170s 754ms/step - loss: 1.1921e-07 - accuracy: 0.5005 - val_loss: 1.1921e-07 - val_accuracy: 0.4978\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.4999 - val_loss: 1.1921e-07 - val_accuracy: 0.4974\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 169s 753ms/step - loss: 1.1921e-07 - accuracy: 0.5012 - val_loss: 1.1921e-07 - val_accuracy: 0.4946\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 176s 780ms/step - loss: 1.1921e-07 - accuracy: 0.5001 - val_loss: 1.1921e-07 - val_accuracy: 0.4993\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.5010 - val_loss: 1.1921e-07 - val_accuracy: 0.4985\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 172s 766ms/step - loss: 1.1921e-07 - accuracy: 0.4984 - val_loss: 1.1921e-07 - val_accuracy: 0.4969\n",
      "Epoch 26/50\n",
      "225/225 [==============================] - 169s 752ms/step - loss: 1.1921e-07 - accuracy: 0.5016 - val_loss: 1.1921e-07 - val_accuracy: 0.4949\n",
      "Epoch 27/50\n",
      "225/225 [==============================] - 173s 767ms/step - loss: 1.1921e-07 - accuracy: 0.4997 - val_loss: 1.1921e-07 - val_accuracy: 0.4965\n",
      "Epoch 28/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.5009 - val_loss: 1.1921e-07 - val_accuracy: 0.4984\n",
      "Epoch 29/50\n",
      "225/225 [==============================] - 170s 754ms/step - loss: 1.1921e-07 - accuracy: 0.4998 - val_loss: 1.1921e-07 - val_accuracy: 0.4935\n",
      "Epoch 30/50\n",
      "225/225 [==============================] - 177s 788ms/step - loss: 1.1921e-07 - accuracy: 0.5011 - val_loss: 1.1921e-07 - val_accuracy: 0.4955\n",
      "Epoch 31/50\n",
      "225/225 [==============================] - 174s 774ms/step - loss: 1.1921e-07 - accuracy: 0.4999 - val_loss: 1.1921e-07 - val_accuracy: 0.4974\n",
      "Epoch 32/50\n",
      "225/225 [==============================] - 176s 783ms/step - loss: 1.1921e-07 - accuracy: 0.5007 - val_loss: 1.1921e-07 - val_accuracy: 0.4979\n",
      "Epoch 33/50\n",
      "225/225 [==============================] - 170s 757ms/step - loss: 1.1921e-07 - accuracy: 0.5004 - val_loss: 1.1921e-07 - val_accuracy: 0.4943\n",
      "Epoch 34/50\n",
      "225/225 [==============================] - 171s 759ms/step - loss: 1.1921e-07 - accuracy: 0.4999 - val_loss: 1.1921e-07 - val_accuracy: 0.4972\n",
      "Epoch 35/50\n",
      "225/225 [==============================] - 170s 757ms/step - loss: 1.1921e-07 - accuracy: 0.4997 - val_loss: 1.1921e-07 - val_accuracy: 0.4994\n",
      "Epoch 36/50\n",
      "225/225 [==============================] - 171s 759ms/step - loss: 1.1921e-07 - accuracy: 0.5004 - val_loss: 1.1921e-07 - val_accuracy: 0.4956\n",
      "Epoch 37/50\n",
      "225/225 [==============================] - 170s 755ms/step - loss: 1.1921e-07 - accuracy: 0.5012 - val_loss: 1.1921e-07 - val_accuracy: 0.4980\n",
      "Epoch 38/50\n",
      "225/225 [==============================] - 175s 777ms/step - loss: 1.1921e-07 - accuracy: 0.4990 - val_loss: 1.1921e-07 - val_accuracy: 0.4943\n",
      "Epoch 39/50\n",
      "225/225 [==============================] - 170s 757ms/step - loss: 1.1921e-07 - accuracy: 0.4994 - val_loss: 1.1921e-07 - val_accuracy: 0.4990\n",
      "Epoch 40/50\n",
      "225/225 [==============================] - 171s 759ms/step - loss: 1.1921e-07 - accuracy: 0.5017 - val_loss: 1.1921e-07 - val_accuracy: 0.4985\n",
      "Epoch 41/50\n",
      "225/225 [==============================] - 170s 758ms/step - loss: 1.1921e-07 - accuracy: 0.5005 - val_loss: 1.1921e-07 - val_accuracy: 0.4955\n",
      "Epoch 42/50\n",
      "225/225 [==============================] - 170s 755ms/step - loss: 1.1921e-07 - accuracy: 0.4988 - val_loss: 1.1921e-07 - val_accuracy: 0.4930\n",
      "Epoch 43/50\n",
      "225/225 [==============================] - 170s 755ms/step - loss: 1.1921e-07 - accuracy: 0.4990 - val_loss: 1.1921e-07 - val_accuracy: 0.4984\n",
      "Epoch 44/50\n",
      "225/225 [==============================] - 170s 757ms/step - loss: 1.1921e-07 - accuracy: 0.5016 - val_loss: 1.1921e-07 - val_accuracy: 0.4939\n",
      "Epoch 45/50\n",
      "225/225 [==============================] - 170s 756ms/step - loss: 1.1921e-07 - accuracy: 0.5003 - val_loss: 1.1921e-07 - val_accuracy: 0.4977\n",
      "Epoch 46/50\n",
      "225/225 [==============================] - 172s 762ms/step - loss: 1.1921e-07 - accuracy: 0.5002 - val_loss: 1.1921e-07 - val_accuracy: 0.4964\n",
      "Epoch 47/50\n",
      "225/225 [==============================] - 169s 752ms/step - loss: 1.1921e-07 - accuracy: 0.4997 - val_loss: 1.1921e-07 - val_accuracy: 0.4941\n",
      "Epoch 48/50\n",
      "225/225 [==============================] - 172s 766ms/step - loss: 1.1921e-07 - accuracy: 0.5016 - val_loss: 1.1921e-07 - val_accuracy: 0.4904\n",
      "Epoch 49/50\n",
      "225/225 [==============================] - 171s 762ms/step - loss: 1.1921e-07 - accuracy: 0.4995 - val_loss: 1.1921e-07 - val_accuracy: 0.5002\n",
      "Epoch 50/50\n",
      "225/225 [==============================] - 180s 799ms/step - loss: 1.1921e-07 - accuracy: 0.5005 - val_loss: 1.1921e-07 - val_accuracy: 0.5006\n",
      "Wall time: 2h 49min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15e1c8a3a58>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                epochs=50,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have got an accuracy of 50% for 50 epochs\n",
    "* I have run this model only with CPU and no graphic card so the time taken is nearly 3 hours\n",
    "* Approximately 17 minutes to run the same model with 50 eopchs in kaggle with GPU enabled\n",
    "\n",
    "\n",
    "* Let's Save the model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    " #### Accuracy can be increased by  \n",
    " * changing number of epochs\n",
    " * changing number of layers in CNN\n",
    " * Changing number of channels\n",
    " * Changing activation functions\n",
    " * Tuning the remaining hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check my work on [kaggle](./https://www.kaggle.com/saitej31) by clicking here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset is taken from kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
